{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0358a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860e1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a4562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"s3://quri-sage-maker-bucket/new_FE_train.feather770\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06623115",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8aea8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 772)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6decb0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>B_30_nunique</th>\n",
       "      <th>B_38_last</th>\n",
       "      <th>B_38_nunique</th>\n",
       "      <th>D_114_last</th>\n",
       "      <th>D_117_last</th>\n",
       "      <th>D_120_last</th>\n",
       "      <th>D_120_nunique</th>\n",
       "      <th>D_66_last</th>\n",
       "      <th>D_66_nunique</th>\n",
       "      <th>D_64_last</th>\n",
       "      <th>D_63_last</th>\n",
       "      <th>D_63_nunique</th>\n",
       "      <th>D_126_nunique</th>\n",
       "      <th>S_2_min</th>\n",
       "      <th>S_27_delta</th>\n",
       "      <th>S_7_delta</th>\n",
       "      <th>S_5_delta</th>\n",
       "      <th>R_5_delta</th>\n",
       "      <th>S_11_delta</th>\n",
       "      <th>D_142_delta</th>\n",
       "      <th>D_43_delta</th>\n",
       "      <th>D_136_delta</th>\n",
       "      <th>S_22_delta</th>\n",
       "      <th>D_110_delta</th>\n",
       "      <th>R_28_delta</th>\n",
       "      <th>B_6_delta</th>\n",
       "      <th>D_102_delta</th>\n",
       "      <th>D_130_delta</th>\n",
       "      <th>D_46_delta</th>\n",
       "      <th>D_137_delta</th>\n",
       "      <th>B_1_delta</th>\n",
       "      <th>D_96_delta</th>\n",
       "      <th>R_14_delta</th>\n",
       "      <th>D_72_delta</th>\n",
       "      <th>S_12_delta</th>\n",
       "      <th>S_13_delta</th>\n",
       "      <th>D_88_delta</th>\n",
       "      <th>B_24_delta</th>\n",
       "      <th>S_25_delta</th>\n",
       "      <th>D_44_delta</th>\n",
       "      <th>B_9_delta</th>\n",
       "      <th>B_17_delta</th>\n",
       "      <th>B_42_delta</th>\n",
       "      <th>D_53_delta</th>\n",
       "      <th>D_58_delta</th>\n",
       "      <th>R_11_delta</th>\n",
       "      <th>D_138_delta</th>\n",
       "      <th>B_4_delta</th>\n",
       "      <th>B_3_delta</th>\n",
       "      <th>R_16_delta</th>\n",
       "      <th>SDist_delta</th>\n",
       "      <th>D_48_delta</th>\n",
       "      <th>D_84_delta</th>\n",
       "      <th>R_7_delta</th>\n",
       "      <th>B_2_delta</th>\n",
       "      <th>D_39_delta</th>\n",
       "      <th>D_41_delta</th>\n",
       "      <th>D_82_delta</th>\n",
       "      <th>D_49_delta</th>\n",
       "      <th>B_16_delta</th>\n",
       "      <th>LT</th>\n",
       "      <th>D_127_quantile</th>\n",
       "      <th>B_22_quantile</th>\n",
       "      <th>D_76_quantile</th>\n",
       "      <th>R_14_quantile</th>\n",
       "      <th>D_121_quantile</th>\n",
       "      <th>S_12_quantile</th>\n",
       "      <th>B_2_quantile</th>\n",
       "      <th>D_56_quantile</th>\n",
       "      <th>D_50_quantile</th>\n",
       "      <th>D_47_quantile</th>\n",
       "      <th>SDist_quantile</th>\n",
       "      <th>R_5_quantile</th>\n",
       "      <th>R_4_quantile</th>\n",
       "      <th>D_87_quantile</th>\n",
       "      <th>D_128_quantile</th>\n",
       "      <th>D_139_quantile</th>\n",
       "      <th>D_45_quantile</th>\n",
       "      <th>D_48_quantile</th>\n",
       "      <th>D_59_quantile</th>\n",
       "      <th>S_18_quantile</th>\n",
       "      <th>D_65_quantile</th>\n",
       "      <th>S_15_quantile</th>\n",
       "      <th>D_141_quantile</th>\n",
       "      <th>S_13_quantile</th>\n",
       "      <th>D_143_quantile</th>\n",
       "      <th>B_13_quantile</th>\n",
       "      <th>D_119_quantile</th>\n",
       "      <th>S_26_quantile</th>\n",
       "      <th>D_43_quantile</th>\n",
       "      <th>D_134_quantile</th>\n",
       "      <th>D_51_quantile</th>\n",
       "      <th>R_7_quantile</th>\n",
       "      <th>B_17_quantile</th>\n",
       "      <th>D_94_quantile</th>\n",
       "      <th>B_3_quantile</th>\n",
       "      <th>D_122_quantile</th>\n",
       "      <th>S_3_quantile</th>\n",
       "      <th>D_91_quantile</th>\n",
       "      <th>D_72_quantile</th>\n",
       "      <th>P_4_quantile</th>\n",
       "      <th>B_40_quantile</th>\n",
       "      <th>R_8_quantile</th>\n",
       "      <th>S_24_quantile</th>\n",
       "      <th>B_10_quantile</th>\n",
       "      <th>D_60_quantile</th>\n",
       "      <th>D_71_quantile</th>\n",
       "      <th>B_42_quantile</th>\n",
       "      <th>D_111_quantile</th>\n",
       "      <th>D_110_quantile</th>\n",
       "      <th>B_9_quantile</th>\n",
       "      <th>D_61_quantile</th>\n",
       "      <th>P_2_quantile</th>\n",
       "      <th>R_26_quantile</th>\n",
       "      <th>D_96_quantile</th>\n",
       "      <th>D_70_quantile</th>\n",
       "      <th>B_14_quantile</th>\n",
       "      <th>D_75_quantile</th>\n",
       "      <th>D_92_quantile</th>\n",
       "      <th>D_74_quantile</th>\n",
       "      <th>R_25_quantile</th>\n",
       "      <th>D_107_quantile</th>\n",
       "      <th>D_42_quantile</th>\n",
       "      <th>R_9_quantile</th>\n",
       "      <th>D_62_quantile</th>\n",
       "      <th>R_27_quantile</th>\n",
       "      <th>D_93_quantile</th>\n",
       "      <th>D_125_quantile</th>\n",
       "      <th>B_32_quantile</th>\n",
       "      <th>B_5_quantile</th>\n",
       "      <th>D_105_quantile</th>\n",
       "      <th>R_24_quantile</th>\n",
       "      <th>D_44_quantile</th>\n",
       "      <th>B_20_mean</th>\n",
       "      <th>R_23_mean</th>\n",
       "      <th>D_59_mean</th>\n",
       "      <th>D_140_mean</th>\n",
       "      <th>D_75_mean</th>\n",
       "      <th>D_106_mean</th>\n",
       "      <th>R_8_mean</th>\n",
       "      <th>D_130_mean</th>\n",
       "      <th>R_26_mean</th>\n",
       "      <th>D_74_mean</th>\n",
       "      <th>R_15_mean</th>\n",
       "      <th>D_52_mean</th>\n",
       "      <th>D_54_mean</th>\n",
       "      <th>R_9_mean</th>\n",
       "      <th>D_121_mean</th>\n",
       "      <th>D_113_mean</th>\n",
       "      <th>S_26_mean</th>\n",
       "      <th>B_14_mean</th>\n",
       "      <th>D_50_mean</th>\n",
       "      <th>S_12_mean</th>\n",
       "      <th>S_15_mean</th>\n",
       "      <th>D_93_mean</th>\n",
       "      <th>B_32_mean</th>\n",
       "      <th>D_73_mean</th>\n",
       "      <th>D_111_mean</th>\n",
       "      <th>D_108_mean</th>\n",
       "      <th>D_77_mean</th>\n",
       "      <th>R_21_mean</th>\n",
       "      <th>SDist_mean</th>\n",
       "      <th>B_9_mean</th>\n",
       "      <th>B_17_mean</th>\n",
       "      <th>D_141_mean</th>\n",
       "      <th>S_5_mean</th>\n",
       "      <th>D_96_mean</th>\n",
       "      <th>B_21_mean</th>\n",
       "      <th>R_1_mean</th>\n",
       "      <th>D_48_mean</th>\n",
       "      <th>D_61_mean</th>\n",
       "      <th>R_5_mean</th>\n",
       "      <th>S_16_mean</th>\n",
       "      <th>D_92_mean</th>\n",
       "      <th>D_91_mean</th>\n",
       "      <th>D_45_mean</th>\n",
       "      <th>B_40_mean</th>\n",
       "      <th>D_110_mean</th>\n",
       "      <th>B_18_mean</th>\n",
       "      <th>D_82_mean</th>\n",
       "      <th>R_27_mean</th>\n",
       "      <th>S_3_mean</th>\n",
       "      <th>D_71_mean</th>\n",
       "      <th>B_42_mean</th>\n",
       "      <th>R_16_mean</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>D_44_mean</th>\n",
       "      <th>D_145_mean</th>\n",
       "      <th>D_122_mean</th>\n",
       "      <th>B_4_mean</th>\n",
       "      <th>D_55_mean</th>\n",
       "      <th>B_15_mean</th>\n",
       "      <th>D_104_mean</th>\n",
       "      <th>D_62_mean</th>\n",
       "      <th>S_6_mean</th>\n",
       "      <th>B_5_mean</th>\n",
       "      <th>D_94_mean</th>\n",
       "      <th>D_72_mean</th>\n",
       "      <th>S_11_mean</th>\n",
       "      <th>D_127_mean</th>\n",
       "      <th>D_142_mean</th>\n",
       "      <th>D_143_mean</th>\n",
       "      <th>D_76_mean</th>\n",
       "      <th>B_41_mean</th>\n",
       "      <th>S_18_mean</th>\n",
       "      <th>D_60_mean</th>\n",
       "      <th>R_3_mean</th>\n",
       "      <th>R_11_mean</th>\n",
       "      <th>B_7_mean</th>\n",
       "      <th>D_118_mean</th>\n",
       "      <th>D_51_mean</th>\n",
       "      <th>R_25_mean</th>\n",
       "      <th>B_2_mean</th>\n",
       "      <th>D_42_mean</th>\n",
       "      <th>B_22_mean</th>\n",
       "      <th>D_47_mean</th>\n",
       "      <th>D_43_mean</th>\n",
       "      <th>B_12_mean</th>\n",
       "      <th>D_83_last</th>\n",
       "      <th>B_9_last</th>\n",
       "      <th>R_5_last</th>\n",
       "      <th>R_11_last</th>\n",
       "      <th>R_16_last</th>\n",
       "      <th>D_50_last</th>\n",
       "      <th>S_25_last</th>\n",
       "      <th>D_111_last</th>\n",
       "      <th>B_4_last</th>\n",
       "      <th>D_76_last</th>\n",
       "      <th>R_2_last</th>\n",
       "      <th>S_24_last</th>\n",
       "      <th>S_3_last</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>D_119_last</th>\n",
       "      <th>B_15_last</th>\n",
       "      <th>R_9_last</th>\n",
       "      <th>B_24_last</th>\n",
       "      <th>D_140_last</th>\n",
       "      <th>D_52_last</th>\n",
       "      <th>SDist_last</th>\n",
       "      <th>D_79_last</th>\n",
       "      <th>B_23_last</th>\n",
       "      <th>D_127_last</th>\n",
       "      <th>D_49_last</th>\n",
       "      <th>B_1_last</th>\n",
       "      <th>D_59_last</th>\n",
       "      <th>S_26_last</th>\n",
       "      <th>R_20_last</th>\n",
       "      <th>B_18_last</th>\n",
       "      <th>S_23_last</th>\n",
       "      <th>...</th>\n",
       "      <th>S_5_std</th>\n",
       "      <th>D_51_std</th>\n",
       "      <th>D_123_std</th>\n",
       "      <th>B_4_std</th>\n",
       "      <th>R_14_std</th>\n",
       "      <th>B_19_std</th>\n",
       "      <th>S_13_std</th>\n",
       "      <th>B_2_std</th>\n",
       "      <th>B_17_std</th>\n",
       "      <th>D_41_std</th>\n",
       "      <th>B_14_std</th>\n",
       "      <th>B_41_std</th>\n",
       "      <th>B_39_std</th>\n",
       "      <th>D_47_std</th>\n",
       "      <th>S_20_std</th>\n",
       "      <th>R_3_std</th>\n",
       "      <th>D_87_std</th>\n",
       "      <th>D_84_std</th>\n",
       "      <th>S_15_std</th>\n",
       "      <th>B_12_std</th>\n",
       "      <th>D_137_std</th>\n",
       "      <th>B_23_std</th>\n",
       "      <th>SDist_std</th>\n",
       "      <th>D_45_std</th>\n",
       "      <th>S_18_std</th>\n",
       "      <th>R_20_std</th>\n",
       "      <th>P_3_std</th>\n",
       "      <th>R_7_std</th>\n",
       "      <th>D_61_std</th>\n",
       "      <th>B_16_std</th>\n",
       "      <th>S_23_std</th>\n",
       "      <th>R_27_min</th>\n",
       "      <th>D_46_min</th>\n",
       "      <th>S_7_min</th>\n",
       "      <th>D_118_min</th>\n",
       "      <th>R_12_min</th>\n",
       "      <th>B_11_min</th>\n",
       "      <th>S_5_min</th>\n",
       "      <th>B_5_min</th>\n",
       "      <th>D_76_min</th>\n",
       "      <th>D_47_min</th>\n",
       "      <th>D_119_min</th>\n",
       "      <th>R_2_min</th>\n",
       "      <th>D_70_min</th>\n",
       "      <th>D_48_min</th>\n",
       "      <th>B_16_min</th>\n",
       "      <th>R_28_min</th>\n",
       "      <th>D_53_min</th>\n",
       "      <th>D_111_min</th>\n",
       "      <th>D_51_min</th>\n",
       "      <th>B_13_min</th>\n",
       "      <th>P_4_min</th>\n",
       "      <th>D_56_min</th>\n",
       "      <th>S_19_min</th>\n",
       "      <th>B_6_min</th>\n",
       "      <th>D_135_min</th>\n",
       "      <th>R_4_min</th>\n",
       "      <th>D_52_min</th>\n",
       "      <th>S_23_min</th>\n",
       "      <th>D_141_min</th>\n",
       "      <th>R_9_min</th>\n",
       "      <th>D_61_min</th>\n",
       "      <th>S_15_min</th>\n",
       "      <th>D_109_min</th>\n",
       "      <th>S_12_min</th>\n",
       "      <th>B_39_min</th>\n",
       "      <th>D_62_min</th>\n",
       "      <th>D_132_min</th>\n",
       "      <th>D_112_min</th>\n",
       "      <th>B_20_min</th>\n",
       "      <th>R_6_min</th>\n",
       "      <th>D_110_min</th>\n",
       "      <th>D_104_min</th>\n",
       "      <th>B_32_min</th>\n",
       "      <th>D_91_min</th>\n",
       "      <th>D_140_min</th>\n",
       "      <th>B_40_min</th>\n",
       "      <th>B_9_min</th>\n",
       "      <th>R_8_min</th>\n",
       "      <th>D_92_min</th>\n",
       "      <th>S_3_min</th>\n",
       "      <th>B_18_min</th>\n",
       "      <th>D_42_min</th>\n",
       "      <th>D_93_min</th>\n",
       "      <th>B_31_min</th>\n",
       "      <th>D_125_min</th>\n",
       "      <th>SDist_min</th>\n",
       "      <th>B_42_min</th>\n",
       "      <th>S_8_min</th>\n",
       "      <th>R_5_min</th>\n",
       "      <th>R_11_min</th>\n",
       "      <th>B_33_min</th>\n",
       "      <th>D_83_min</th>\n",
       "      <th>R_23_min</th>\n",
       "      <th>D_127_min</th>\n",
       "      <th>S_25_min</th>\n",
       "      <th>D_121_min</th>\n",
       "      <th>D_94_min</th>\n",
       "      <th>D_65_min</th>\n",
       "      <th>B_8_min</th>\n",
       "      <th>S_11_min</th>\n",
       "      <th>D_134_min</th>\n",
       "      <th>B_17_min</th>\n",
       "      <th>D_39_min</th>\n",
       "      <th>S_6_min</th>\n",
       "      <th>D_45_min</th>\n",
       "      <th>D_122_min</th>\n",
       "      <th>D_59_min</th>\n",
       "      <th>D_84_min</th>\n",
       "      <th>D_42_last_round2</th>\n",
       "      <th>B_17_last_round2</th>\n",
       "      <th>B_3_last_round2</th>\n",
       "      <th>D_62_last_round2</th>\n",
       "      <th>B_18_last_round2</th>\n",
       "      <th>R_12_last_round2</th>\n",
       "      <th>D_133_last_round2</th>\n",
       "      <th>D_46_last_round2</th>\n",
       "      <th>B_2_last_round2</th>\n",
       "      <th>S_12_last_round2</th>\n",
       "      <th>B_11_last_round2</th>\n",
       "      <th>D_58_last_round2</th>\n",
       "      <th>D_43_last_round2</th>\n",
       "      <th>B_15_last_round2</th>\n",
       "      <th>SDist_last_round2</th>\n",
       "      <th>B_10_last_round2</th>\n",
       "      <th>B_1_last_round2</th>\n",
       "      <th>S_25_last_round2</th>\n",
       "      <th>D_112_last_round2</th>\n",
       "      <th>S_3_last_round2</th>\n",
       "      <th>R_1_last_round2</th>\n",
       "      <th>D_47_last_round2</th>\n",
       "      <th>D_50_last_round2</th>\n",
       "      <th>D_55_last_round2</th>\n",
       "      <th>D_61_last_round2</th>\n",
       "      <th>B_24_last_round2</th>\n",
       "      <th>R_14_last_round2</th>\n",
       "      <th>D_76_last_round2</th>\n",
       "      <th>B_5_last_round2</th>\n",
       "      <th>S_23_last_round2</th>\n",
       "      <th>D_48_last_round2</th>\n",
       "      <th>D_45_last_round2</th>\n",
       "      <th>B_42_last_round2</th>\n",
       "      <th>B_6_last_round2</th>\n",
       "      <th>D_110_last_round2</th>\n",
       "      <th>S_5_last_round2</th>\n",
       "      <th>S_16_last_round2</th>\n",
       "      <th>R_7_last_round2</th>\n",
       "      <th>R_27_last_round2</th>\n",
       "      <th>B_23_last_round2</th>\n",
       "      <th>D_60_last_round2</th>\n",
       "      <th>B_9_last_round2</th>\n",
       "      <th>B_28_last_round2</th>\n",
       "      <th>D_88_last_round2</th>\n",
       "      <th>D_69_last_round2</th>\n",
       "      <th>D_121_last_round2</th>\n",
       "      <th>S_24_last_round2</th>\n",
       "      <th>D_56_last_round2</th>\n",
       "      <th>D_119_last_round2</th>\n",
       "      <th>D_41_last_round2</th>\n",
       "      <th>B_39_last_round2</th>\n",
       "      <th>D_52_last_round2</th>\n",
       "      <th>P_2_max-P_2_min</th>\n",
       "      <th>D_44_last-B_37_last</th>\n",
       "      <th>D_44_last-P_2_last</th>\n",
       "      <th>D_44_last-P_3_last</th>\n",
       "      <th>B_23_last-P_2_last</th>\n",
       "      <th>B_23_last-P_3_last</th>\n",
       "      <th>P_3_last-D_131_last</th>\n",
       "      <th>P_2_last-D_131_last</th>\n",
       "      <th>B_17_last-P_2_last</th>\n",
       "      <th>B_17_last-P_3_last</th>\n",
       "      <th>B_14_last-P_2_last</th>\n",
       "      <th>B_11_last-P_2_last</th>\n",
       "      <th>B_14_last-P_3_last</th>\n",
       "      <th>B_11_last-P_3_last</th>\n",
       "      <th>B_2_last-P_2_last</th>\n",
       "      <th>B_2_last-P_3_last</th>\n",
       "      <th>D_48_last-B_2_last</th>\n",
       "      <th>D_48_last-B_1_last</th>\n",
       "      <th>D_42_last-P_2_last</th>\n",
       "      <th>D_42_last-P_3_last</th>\n",
       "      <th>D_39_last-P_2_last</th>\n",
       "      <th>D_39_last-P_3_last</th>\n",
       "      <th>B_9_last-P_2_last</th>\n",
       "      <th>B_9_last-P_3_last</th>\n",
       "      <th>B_4_last-D_62_last</th>\n",
       "      <th>P_3_last-S_23_last</th>\n",
       "      <th>P_2_last-S_23_last</th>\n",
       "      <th>P_3_last-S_16_last</th>\n",
       "      <th>P_2_last-S_16_last</th>\n",
       "      <th>S_5_last_first_diff</th>\n",
       "      <th>R_14_last_first_diff</th>\n",
       "      <th>SDist_last_first_diff</th>\n",
       "      <th>D_112_last_first_diff</th>\n",
       "      <th>B_3_last_first_diff</th>\n",
       "      <th>B_9_last_first_diff</th>\n",
       "      <th>D_144_last_first_diff</th>\n",
       "      <th>B_15_last_first_diff</th>\n",
       "      <th>B_8_last_first_diff</th>\n",
       "      <th>D_102_last_first_diff</th>\n",
       "      <th>D_41_last_first_diff</th>\n",
       "      <th>D_48_last_first_diff</th>\n",
       "      <th>D_69_last_first_diff</th>\n",
       "      <th>R_12_last_first_diff</th>\n",
       "      <th>D_50_last_first_diff</th>\n",
       "      <th>B_17_last_first_diff</th>\n",
       "      <th>D_141_last_first_diff</th>\n",
       "      <th>D_88_last_first_diff</th>\n",
       "      <th>P_3_last_first_diff</th>\n",
       "      <th>B_18_last_first_diff</th>\n",
       "      <th>D_55_last_first_diff</th>\n",
       "      <th>D_58_last_first_diff</th>\n",
       "      <th>B_5_last_first_diff</th>\n",
       "      <th>D_58_last_mean_diff</th>\n",
       "      <th>B_7_last_mean_diff</th>\n",
       "      <th>B_14_last_mean_diff</th>\n",
       "      <th>D_41_last_mean_diff</th>\n",
       "      <th>D_73_last_mean_diff</th>\n",
       "      <th>B_1_last_mean_diff</th>\n",
       "      <th>D_55_last_mean_diff</th>\n",
       "      <th>D_48_last_mean_diff</th>\n",
       "      <th>S_3_last_mean_diff</th>\n",
       "      <th>R_14_last_mean_diff</th>\n",
       "      <th>B_15_last_mean_diff</th>\n",
       "      <th>B_23_last_mean_diff</th>\n",
       "      <th>D_46_last_mean_diff</th>\n",
       "      <th>B_9_last_mean_diff</th>\n",
       "      <th>B_5_last_mean_diff</th>\n",
       "      <th>D_131_last_mean_diff</th>\n",
       "      <th>SDist_last_mean_diff</th>\n",
       "      <th>S_12_last_mean_diff</th>\n",
       "      <th>S_17_last_mean_diff</th>\n",
       "      <th>D_121_last_mean_diff</th>\n",
       "      <th>D_110_last_mean_diff</th>\n",
       "      <th>D_141_last_mean_diff</th>\n",
       "      <th>D_144_last_mean_diff</th>\n",
       "      <th>S_24_last_mean_diff</th>\n",
       "      <th>D_50_last_mean_diff</th>\n",
       "      <th>B_6_last_mean_diff</th>\n",
       "      <th>B_10_last_mean_diff</th>\n",
       "      <th>D_53_last_mean_diff</th>\n",
       "      <th>B_28_last_mean_diff</th>\n",
       "      <th>S_22_last_mean_diff</th>\n",
       "      <th>B_3_last_mean_diff</th>\n",
       "      <th>D_56_last_mean_diff</th>\n",
       "      <th>D_130_last_mean_diff</th>\n",
       "      <th>S_7_last_mean_diff</th>\n",
       "      <th>total_data_count</th>\n",
       "      <th>total_data_last</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448663</th>\n",
       "      <td>8807060616933747883</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399414</td>\n",
       "      <td>0.021545</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.052002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018936</td>\n",
       "      <td>-0.004669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.139771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240967</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006424</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.006565</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.029633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721191</td>\n",
       "      <td>0.190552</td>\n",
       "      <td>0.816406</td>\n",
       "      <td>0.174683</td>\n",
       "      <td>0.108459</td>\n",
       "      <td>0.61084</td>\n",
       "      <td>30.53125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069885</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.007835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00602</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.157837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086609</td>\n",
       "      <td>0.296631</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.162476</td>\n",
       "      <td>0.849121</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.435791</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.844727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.46875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.724121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.012787</td>\n",
       "      <td>0.108704</td>\n",
       "      <td>0.20813</td>\n",
       "      <td>7.152344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.846191</td>\n",
       "      <td>0.405029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.65625</td>\n",
       "      <td>0.042816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.147339</td>\n",
       "      <td>0.166626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06842</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>0.158447</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230713</td>\n",
       "      <td>0.850098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.152344</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>0.436035</td>\n",
       "      <td>0.230713</td>\n",
       "      <td>0.011444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.078125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.153809</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.723633</td>\n",
       "      <td>1.384766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.872559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.040649</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254639</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109314</td>\n",
       "      <td>0.976074</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972168</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.879883</td>\n",
       "      <td>0.741211</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196533</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.304932</td>\n",
       "      <td>28</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.13855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.506348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.511719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.5</td>\n",
       "      <td>0.091003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.577148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.28125</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>9.625</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.375488</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.099792</td>\n",
       "      <td>0.711426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.596191</td>\n",
       "      <td>0.716797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17041</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196533</td>\n",
       "      <td>0.13208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.087158</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.149048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.715332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054504</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.409912</td>\n",
       "      <td>0.810059</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>0.26001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109985</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>0.97998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160034</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.620117</td>\n",
       "      <td>0.109985</td>\n",
       "      <td>0.140015</td>\n",
       "      <td>0.119995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.140015</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.080017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.320068</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.970215</td>\n",
       "      <td>0.170044</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>0.085022</td>\n",
       "      <td>-0.311523</td>\n",
       "      <td>-0.879883</td>\n",
       "      <td>-0.749512</td>\n",
       "      <td>-0.849121</td>\n",
       "      <td>-0.71875</td>\n",
       "      <td>0.749512</td>\n",
       "      <td>0.879883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.808105</td>\n",
       "      <td>-0.624023</td>\n",
       "      <td>-0.677734</td>\n",
       "      <td>-0.493896</td>\n",
       "      <td>-0.067749</td>\n",
       "      <td>0.062378</td>\n",
       "      <td>-0.722656</td>\n",
       "      <td>-0.215332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.125</td>\n",
       "      <td>21.25</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.494873</td>\n",
       "      <td>8.570312</td>\n",
       "      <td>0.61084</td>\n",
       "      <td>0.741211</td>\n",
       "      <td>0.746582</td>\n",
       "      <td>0.876953</td>\n",
       "      <td>0.030685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.530273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005318</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>-0.00036</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>-0.006802</td>\n",
       "      <td>-0.967773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.227173</td>\n",
       "      <td>-0.006817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.101135</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>0.012131</td>\n",
       "      <td>0.059082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>-0.042999</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>-0.03244</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.65625</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.51123</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>0.047821</td>\n",
       "      <td>-0.00152</td>\n",
       "      <td>-0.004089</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>0.393066</td>\n",
       "      <td>-0.003586</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025589</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                customer_ID  B_30_nunique  B_38_last  B_38_nunique  D_114_last  D_117_last  D_120_last  D_120_nunique  D_66_last  D_66_nunique  D_64_last  D_63_last  D_63_nunique  D_126_nunique  S_2_min  S_27_delta  S_7_delta  S_5_delta  R_5_delta  S_11_delta  D_142_delta  D_43_delta  D_136_delta  S_22_delta  D_110_delta  R_28_delta  B_6_delta  D_102_delta  D_130_delta  D_46_delta  D_137_delta  B_1_delta  D_96_delta  R_14_delta  D_72_delta  S_12_delta  S_13_delta  D_88_delta  B_24_delta  S_25_delta  D_44_delta  B_9_delta  B_17_delta  B_42_delta  D_53_delta  D_58_delta  R_11_delta  D_138_delta  B_4_delta  B_3_delta  R_16_delta  SDist_delta  D_48_delta  D_84_delta  R_7_delta  B_2_delta  D_39_delta  D_41_delta  D_82_delta  D_49_delta  B_16_delta    LT  D_127_quantile  B_22_quantile  D_76_quantile  R_14_quantile  D_121_quantile  S_12_quantile  B_2_quantile  D_56_quantile  D_50_quantile  D_47_quantile  SDist_quantile  R_5_quantile  R_4_quantile  D_87_quantile  D_128_quantile  \\\n",
       "448663  8807060616933747883             1          2             3           2           5           1              1          1             1          3          3             1              2        0    0.399414   0.021545   0.023804        0.0        -1.0          0.0   -0.052002          0.0    0.000586          0.0         0.0  -0.018936    -0.004669          0.0   -0.139771          0.0   0.233521         0.0         0.0         0.0    0.240967       328.0         0.0   -0.000232    0.005005         0.0   0.182373         0.0         0.0   -0.006424   -0.004265         1.0          0.0        4.0  -0.006565        -1.0         20.0   -0.029633         0.0        0.0   0.001508        22.0         0.0         0.0         0.0         0.0  13.0             0.0            0.0       0.010345            0.0        0.721191       0.190552      0.816406       0.174683       0.108459        0.61084        30.53125           0.0           0.0           -1.0        1.003906   \n",
       "\n",
       "        D_139_quantile  D_45_quantile  D_48_quantile  D_59_quantile  S_18_quantile  D_65_quantile  S_15_quantile  D_141_quantile  S_13_quantile  D_143_quantile  B_13_quantile  D_119_quantile  S_26_quantile  D_43_quantile  D_134_quantile  D_51_quantile  R_7_quantile  B_17_quantile  D_94_quantile  B_3_quantile  D_122_quantile  S_3_quantile  D_91_quantile  D_72_quantile  P_4_quantile  B_40_quantile  R_8_quantile  S_24_quantile  B_10_quantile  D_60_quantile  D_71_quantile  B_42_quantile  D_111_quantile  D_110_quantile  B_9_quantile  D_61_quantile  P_2_quantile  R_26_quantile  D_96_quantile  D_70_quantile  B_14_quantile  D_75_quantile  D_92_quantile  D_74_quantile  R_25_quantile  D_107_quantile  D_42_quantile  R_9_quantile  D_62_quantile  R_27_quantile  D_93_quantile  D_125_quantile  B_32_quantile  B_5_quantile  D_105_quantile  R_24_quantile  D_44_quantile  B_20_mean  R_23_mean  D_59_mean  D_140_mean  D_75_mean  D_106_mean  R_8_mean  D_130_mean  R_26_mean  D_74_mean  R_15_mean  D_52_mean  \\\n",
       "448663             0.0       0.069885       0.123718           26.0            0.0            0.0            8.0             0.0            0.0             0.0       0.016586        0.720703       0.009453       0.007835             0.0            1.0           0.0            0.0            0.0       0.00602             6.0      0.157837            0.0            0.0           0.0       0.021561           0.0       0.086609       0.296631       0.007687       0.012947            0.0            -1.0             0.0      0.006306       0.162476      0.849121           -1.0            0.0            0.0       0.007114            0.0            0.0            0.0            0.0             1.0            0.0          -1.0       0.435791       1.003906            1.0             0.0            0.0      0.007668        0.844727            0.0            0.0        0.0        0.0   26.46875         0.0        0.0        -1.0       0.0         0.0       -1.0        0.0        0.0   0.242432   \n",
       "\n",
       "        D_54_mean  R_9_mean  D_121_mean  D_113_mean  S_26_mean  B_14_mean  D_50_mean  S_12_mean  S_15_mean  D_93_mean  B_32_mean  D_73_mean  D_111_mean  D_108_mean  D_77_mean  R_21_mean  SDist_mean  B_9_mean  B_17_mean  D_141_mean  S_5_mean  D_96_mean  B_21_mean  R_1_mean  D_48_mean  D_61_mean  R_5_mean  S_16_mean  D_92_mean  D_91_mean  D_45_mean  B_40_mean  D_110_mean  B_18_mean  D_82_mean  R_27_mean  S_3_mean  D_71_mean  B_42_mean  R_16_mean  P_2_mean  D_44_mean  D_145_mean  D_122_mean  B_4_mean  D_55_mean  B_15_mean  D_104_mean  D_62_mean  S_6_mean  B_5_mean  D_94_mean  D_72_mean  S_11_mean  D_127_mean  D_142_mean  D_143_mean  D_76_mean  B_41_mean  S_18_mean  D_60_mean  R_3_mean  R_11_mean  B_7_mean  D_118_mean  D_51_mean  R_25_mean  B_2_mean  D_42_mean  B_22_mean  D_47_mean  D_43_mean  B_12_mean  D_83_last  B_9_last  R_5_last  R_11_last  R_16_last  D_50_last  S_25_last  D_111_last  B_4_last  D_76_last  R_2_last  S_24_last  S_3_last  P_2_last  D_119_last  B_15_last  R_9_last  \\\n",
       "448663        1.0      -1.0    0.724121         0.0   0.037598   0.012787   0.108704    0.20813   7.152344        1.0        0.0        0.0        -1.0   -0.846191   0.405029        0.0    29.65625  0.042816        0.0         0.0  0.006958        0.0   0.004944  0.004181   0.147339   0.166626       0.0   0.004681        0.0        0.0    0.06842   0.021393         0.0        1.0       -1.0   1.003906  0.158447   0.012291        0.0   0.230713  0.850098        0.0         0.0         6.0  5.152344   0.182983   0.004517    0.540039   0.436035  0.230713  0.011444        0.0        0.0  14.078125         0.0         0.0         0.0   0.014938        0.0        0.0   0.052307       1.0   0.153809  0.026718    0.723633   1.384766        0.0  0.872559        0.0        0.0   0.609375   0.040649   0.015091          0  0.254639         0          1          0   0.109314   0.976074          -1         9   0.032471         0   0.972168    0.1604  0.879883    0.741211   0.009232        -1   \n",
       "\n",
       "        B_24_last  D_140_last  D_52_last  SDist_last  D_79_last  B_23_last  D_127_last  D_49_last  B_1_last  D_59_last  S_26_last  R_20_last  B_18_last  S_23_last  ...   S_5_std  D_51_std  D_123_std   B_4_std  R_14_std  B_19_std  S_13_std   B_2_std  B_17_std  D_41_std  B_14_std  B_41_std  B_39_std  D_47_std  S_20_std   R_3_std  D_87_std  D_84_std  S_15_std  B_12_std  D_137_std  B_23_std  SDist_std  D_45_std  S_18_std  R_20_std   P_3_std  R_7_std  D_61_std  B_16_std  S_23_std  R_27_min  D_46_min   S_7_min  D_118_min  R_12_min  B_11_min   S_5_min   B_5_min  D_76_min  D_47_min  D_119_min  R_2_min  D_70_min  D_48_min  B_16_min  R_28_min  D_53_min  D_111_min  D_51_min  B_13_min  P_4_min  D_56_min  S_19_min   B_6_min  D_135_min  R_4_min  D_52_min  S_23_min  D_141_min  R_9_min  D_61_min  S_15_min  D_109_min  S_12_min  B_39_min  D_62_min  D_132_min  D_112_min  B_20_min  R_6_min  D_110_min  D_104_min  B_32_min  D_91_min  D_140_min  B_40_min   B_9_min  R_8_min  D_92_min   S_3_min  B_18_min  \\\n",
       "448663   0.003036           0   0.196533        29.0          0   0.030884           0         -1  0.304932         28   0.009453          0        1.0    0.13855  ...  0.007793  0.506348        0.0  2.511719       0.0       0.0     276.5  0.091003       0.0       0.0   0.01889       0.0       0.0  0.007042       0.0  0.577148       0.0       0.0   1.28125  0.004059        0.0  0.005337      9.625   0.00975       0.0       0.0  0.061798      0.0  0.059631  0.375488  0.002731  1.000977  0.327148  0.099792   0.711426       1.0  0.000889  0.000261  0.000414  0.005787  0.596191   0.716797        0         0  0.079895         0         0  0.000109         -1         1   0.00125      0.0   0.17041  0.000607  0.163574         -1        0  0.196533   0.13208        0.0       -1  0.087158         5          0  0.184814       0.0  0.431885        0.0        1.0         0      0.0        0.0        0.0         0         0          0  0.011398  0.000069        0         0  0.149048       1.0   \n",
       "\n",
       "        D_42_min  D_93_min  B_31_min  D_125_min  SDist_min  B_42_min  S_8_min  R_5_min  R_11_min  B_33_min  D_83_min  R_23_min  D_127_min  S_25_min  D_121_min  D_94_min  D_65_min   B_8_min  S_11_min  D_134_min  B_17_min  D_39_min  S_6_min  D_45_min  D_122_min  D_59_min  D_84_min  D_42_last_round2  B_17_last_round2  B_3_last_round2  D_62_last_round2  B_18_last_round2  R_12_last_round2  D_133_last_round2  D_46_last_round2  B_2_last_round2  S_12_last_round2  B_11_last_round2  D_58_last_round2  D_43_last_round2  B_15_last_round2  SDist_last_round2  B_10_last_round2  B_1_last_round2  S_25_last_round2  D_112_last_round2  S_3_last_round2  R_1_last_round2  D_47_last_round2  D_50_last_round2  D_55_last_round2  D_61_last_round2  B_24_last_round2  R_14_last_round2  D_76_last_round2  B_5_last_round2  S_23_last_round2  D_48_last_round2  D_45_last_round2  B_42_last_round2  B_6_last_round2  D_110_last_round2  S_5_last_round2  S_16_last_round2  R_7_last_round2  R_27_last_round2  B_23_last_round2  \\\n",
       "448663       0.0         1         1          0        9.0       0.0        0        0         0         1         0         0          0  0.969238   0.715332         0         0  0.999512        12        0.0       0.0         0        0  0.054504          6        24         0               0.0               0.0              0.0          0.429932               1.0               1.0           0.010002          0.409912         0.810059          0.429932           0.26001               0.0          0.109985          0.010002               29.0          0.300049         0.300049           0.97998                1.0         0.160034         0.010002          0.620117          0.109985          0.140015          0.119995               0.0               0.0          0.029999         0.010002          0.140015          0.090027          0.080017               0.0         0.290039                0.0         0.029999               0.0              0.0               1.0          0.029999   \n",
       "\n",
       "        D_60_last_round2  B_9_last_round2  B_28_last_round2  D_88_last_round2  D_69_last_round2  D_121_last_round2  S_24_last_round2  D_56_last_round2  D_119_last_round2  D_41_last_round2  B_39_last_round2  D_52_last_round2  P_2_max-P_2_min  D_44_last-B_37_last  D_44_last-P_2_last  D_44_last-P_3_last  B_23_last-P_2_last  B_23_last-P_3_last  P_3_last-D_131_last  P_2_last-D_131_last  B_17_last-P_2_last  B_17_last-P_3_last  B_14_last-P_2_last  B_11_last-P_2_last  B_14_last-P_3_last  B_11_last-P_3_last  B_2_last-P_2_last  B_2_last-P_3_last  D_48_last-B_2_last  D_48_last-B_1_last  D_42_last-P_2_last  D_42_last-P_3_last  D_39_last-P_2_last  D_39_last-P_3_last  B_9_last-P_2_last  B_9_last-P_3_last  B_4_last-D_62_last  P_3_last-S_23_last  P_2_last-S_23_last  P_3_last-S_16_last  P_2_last-S_16_last  S_5_last_first_diff  R_14_last_first_diff  SDist_last_first_diff  D_112_last_first_diff  B_3_last_first_diff  B_9_last_first_diff  D_144_last_first_diff  B_15_last_first_diff  B_8_last_first_diff  \\\n",
       "448663          0.320068             0.25          0.099976               0.0               0.0           0.740234          0.970215          0.170044           0.740234               0.0               0.0          0.199951         0.085022            -0.311523           -0.879883           -0.749512           -0.849121            -0.71875             0.749512             0.879883                 0.0                 0.0           -0.808105           -0.624023           -0.677734           -0.493896          -0.067749           0.062378           -0.722656           -0.215332                 0.0                 0.0              21.125               21.25             -0.625          -0.494873            8.570312             0.61084            0.741211            0.746582            0.876953             0.030685                   0.0              -1.530273                    0.0            -0.005318             0.104492               -0.00036              0.002781            -0.006802   \n",
       "\n",
       "        D_102_last_first_diff  D_41_last_first_diff  D_48_last_first_diff  D_69_last_first_diff  R_12_last_first_diff  D_50_last_first_diff  B_17_last_first_diff  D_141_last_first_diff  D_88_last_first_diff  P_3_last_first_diff  B_18_last_first_diff  D_55_last_first_diff  D_58_last_first_diff  B_5_last_first_diff  D_58_last_mean_diff  B_7_last_mean_diff  B_14_last_mean_diff  D_41_last_mean_diff  D_73_last_mean_diff  B_1_last_mean_diff  D_55_last_mean_diff  D_48_last_mean_diff  S_3_last_mean_diff  R_14_last_mean_diff  B_15_last_mean_diff  B_23_last_mean_diff  D_46_last_mean_diff  B_9_last_mean_diff  B_5_last_mean_diff  D_131_last_mean_diff  SDist_last_mean_diff  S_12_last_mean_diff  S_17_last_mean_diff  D_121_last_mean_diff  D_110_last_mean_diff  D_141_last_mean_diff  D_144_last_mean_diff  S_24_last_mean_diff  D_50_last_mean_diff  B_6_last_mean_diff  B_10_last_mean_diff  D_53_last_mean_diff  B_28_last_mean_diff  S_22_last_mean_diff  B_3_last_mean_diff  D_56_last_mean_diff  \\\n",
       "448663              -0.967773                   0.0             -0.227173             -0.006817                   0.0              0.003246                   0.0                    0.0                   0.0             0.152588                   0.0             -0.101135              0.000535            -0.023438            -0.000822            0.012131             0.059082                  0.0                  0.0            0.264893            -0.042999            -0.057831            0.001945                  0.0             0.004715             0.011688             -0.03244            0.211914           -0.002934                   0.0              -0.65625             0.223022             0.001225              0.013313                   0.0                   0.0              0.001416              0.51123              0.00061            0.047821             -0.00152            -0.004089             0.039398             0.393066           -0.003586            -0.000432   \n",
       "\n",
       "        D_130_last_mean_diff  S_7_last_mean_diff  total_data_count  total_data_last  target  \n",
       "448663                   0.0            0.025589            2292.0            176.0       0  \n",
       "\n",
       "[1 rows x 772 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada91cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn!=0.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from lightgbm) (1.0.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from lightgbm) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from lightgbm) (0.37.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.0.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d625b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DartEarlyStopping(object):\n",
    "\n",
    "    def __init__(self, data_name, monitor_metric, stopping_round):\n",
    "        self.data_name = data_name\n",
    "        self.monitor_metric = monitor_metric\n",
    "        self.stopping_round = stopping_round\n",
    "        self.best_score = None\n",
    "        self.best_model = None\n",
    "        self.best_score_list = []\n",
    "        self.best_iter = 0\n",
    "\n",
    "    def _is_higher_score(self, metric_score, is_higher_better):\n",
    "        if self.best_score is None:\n",
    "            return True\n",
    "        return (self.best_score < metric_score) if is_higher_better else (self.best_score > metric_score)\n",
    "\n",
    "    def _deepcopy(self, x):\n",
    "    \n",
    "        return pickle.loads(pickle.dumps(x))\n",
    "\n",
    "    def __call__(self, env):\n",
    "        evals = env.evaluation_result_list\n",
    "        for data, metric, score, is_higher_better in evals:\n",
    "            if data != self.data_name or metric != self.monitor_metric:\n",
    "                continue\n",
    "            if not self._is_higher_score(score, is_higher_better):\n",
    "                if env.iteration - self.best_iter > self.stopping_round:\n",
    "            \n",
    "                    eval_result_str = '\\t'.join([lgb.callback._format_eval_result(x) for x in self.best_score_list])\n",
    "                    lgb.basic._log_info(f\"Early stopping, best iteration is:\\n[{self.best_iter+1}]\\t{eval_result_str}\") \n",
    "                    lgb.basic._log_info(f\"You can get best model by \\\"DartEarlyStopping.best_model\\\"\")\n",
    "                    raise lgb.callback.EarlyStopException(self.best_iter, self.best_score_list)\n",
    "                return\n",
    "        \n",
    "            self.best_model = self._deepcopy(env.model)\n",
    "            self.best_iter = env.iteration\n",
    "            self.best_score_list = evals\n",
    "            self.best_score = score\n",
    "            return\n",
    "        raise ValueError(\"monitoring metric not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c1c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 770 features...\n",
      "[LightGBM] [Info] [cross_entropy_lambda:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.029747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 133051\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 768\n",
      "[LightGBM] [Info] [cross_entropy_lambda:BoostFromScore]: havg = 0.258933 -> initscore = -1.218928\n",
      "[LightGBM] [Info] Start training from score -1.218928\n",
      "[500]\ttraining's binary_logloss: 0.769104\ttraining's amex_metric: 0.783184\tvalid_1's binary_logloss: 0.821098\tvalid_1's amex_metric: 0.76553\n",
      "[1000]\ttraining's binary_logloss: 0.846682\ttraining's amex_metric: 0.796725\tvalid_1's binary_logloss: 0.928745\tvalid_1's amex_metric: 0.774083\n",
      "[1500]\ttraining's binary_logloss: 0.848296\ttraining's amex_metric: 0.809517\tvalid_1's binary_logloss: 0.977396\tvalid_1's amex_metric: 0.781235\n",
      "[2000]\ttraining's binary_logloss: 0.810652\ttraining's amex_metric: 0.819644\tvalid_1's binary_logloss: 0.974177\tvalid_1's amex_metric: 0.785509\n",
      "[2500]\ttraining's binary_logloss: 0.757131\ttraining's amex_metric: 0.831098\tvalid_1's binary_logloss: 0.963271\tvalid_1's amex_metric: 0.788765\n",
      "[3000]\ttraining's binary_logloss: 0.714335\ttraining's amex_metric: 0.840294\tvalid_1's binary_logloss: 0.961154\tvalid_1's amex_metric: 0.790089\n",
      "[3500]\ttraining's binary_logloss: 0.668364\ttraining's amex_metric: 0.849362\tvalid_1's binary_logloss: 0.948473\tvalid_1's amex_metric: 0.791024\n",
      "[4000]\ttraining's binary_logloss: 0.632252\ttraining's amex_metric: 0.857923\tvalid_1's binary_logloss: 0.952507\tvalid_1's amex_metric: 0.792187\n",
      "[4500]\ttraining's binary_logloss: 0.579503\ttraining's amex_metric: 0.867824\tvalid_1's binary_logloss: 0.94504\tvalid_1's amex_metric: 0.793513\n",
      "[5000]\ttraining's binary_logloss: 0.540738\ttraining's amex_metric: 0.875632\tvalid_1's binary_logloss: 0.946083\tvalid_1's amex_metric: 0.793618\n",
      "[5500]\ttraining's binary_logloss: 0.496908\ttraining's amex_metric: 0.88397\tvalid_1's binary_logloss: 0.947687\tvalid_1's amex_metric: 0.794157\n",
      "[6000]\ttraining's binary_logloss: 0.458397\ttraining's amex_metric: 0.890699\tvalid_1's binary_logloss: 0.944878\tvalid_1's amex_metric: 0.794347\n",
      "[6500]\ttraining's binary_logloss: 0.41971\ttraining's amex_metric: 0.898111\tvalid_1's binary_logloss: 0.950044\tvalid_1's amex_metric: 0.794997\n",
      "[7000]\ttraining's binary_logloss: 0.38464\ttraining's amex_metric: 0.905044\tvalid_1's binary_logloss: 0.949474\tvalid_1's amex_metric: 0.795938\n",
      "[7500]\ttraining's binary_logloss: 0.348275\ttraining's amex_metric: 0.911272\tvalid_1's binary_logloss: 0.9472\tvalid_1's amex_metric: 0.795217\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    seed = 41\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train):\n",
    "    \n",
    "    features = [col for col in train.columns if col not in ['customer_ID', 'target']]\n",
    "    \n",
    "    cat_features = cat_features = [\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    \n",
    "    print(len(features))\n",
    "    oof=[]\n",
    "    VER = 1\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'cross_entropy_lambda',\n",
    "        'metric': CFG.metric,\n",
    "        'boosting': CFG.boosting_type,\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.2,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40,\n",
    "        }\n",
    "    \n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train,categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val,categorical_feature = cat_features)\n",
    "        \n",
    "        des = DartEarlyStopping(\"valid_1\", \"amex_metric\", 2000)\n",
    "        \n",
    "        model = lgb.train(\n",
    "           params = params,\n",
    "           train_set = lgb_train,\n",
    "           num_boost_round = 12500,\n",
    "           valid_sets = [lgb_train, lgb_valid],\n",
    "           early_stopping_rounds = 2000,\n",
    "           verbose_eval = 500,\n",
    "           feval = lgb_amex_metric,\n",
    "           callbacks=[des],\n",
    "           )    \n",
    "        \n",
    "        model=lgb.Booster(model_file=f'Models/lgbm_fold{fold}.txt')\n",
    "        \n",
    "        model = des.best_model\n",
    "        \n",
    "        model.save_model(f'Models/lgbm_fold{fold}_seed{CFG.seed}.txt')\n",
    "        \n",
    "        #Save best model\n",
    "       # joblib.dump(model, f'Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        \n",
    "        # Predict validation\n",
    "        preds = model.predict(x_val)\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, preds)\n",
    "        \n",
    "        # SAVE OOF\n",
    "        df = train.loc[val_ind, ['customer_ID','target'] ].copy()\n",
    "        df['oof_pred_lgbm'] = preds\n",
    "        oof.append( df )\n",
    "        \n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    \n",
    "    oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
    "    \n",
    "    val_score = amex_metric(oof.target.values, oof.oof_pred_lgbm.values)\n",
    "    print(f\"Overall Amex metric Score: {val_score}\")\n",
    "    \n",
    "    del train\n",
    "    gc.collect()\n",
    "    \n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    \n",
    "    path_train = \"s3://quri-sage-maker-bucket/train.parquet\"\n",
    "    oof_lgbm = pd.read_parquet(path_train, columns=['customer_ID']).drop_duplicates()\n",
    "    oof_lgbm['customer_ID_hash'] = oof_lgbm['customer_ID'].str[-16:].apply(int, base=16).astype('int64')\n",
    "    oof_lgbm = oof_lgbm.set_index('customer_ID_hash')\n",
    "    oof_lgbm = oof_lgbm.merge(oof, left_index=True, right_index=True)\n",
    "    oof_lgbm = oof_lgbm.sort_index().reset_index(drop=True)\n",
    "    oof_lgbm.to_csv(f'oof_lgbm_v{VER}.csv',index=False)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train_and_evaluate(train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgbm = pd.read_csv('oof_lgbm_v1.csv')\n",
    "oof_lgbm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c24ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18a686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
