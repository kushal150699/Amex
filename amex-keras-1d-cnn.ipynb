{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f245fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T04:29:34.112597Z",
     "iopub.status.busy": "2022-08-12T04:29:34.111705Z",
     "iopub.status.idle": "2022-08-12T04:30:14.488093Z",
     "shell.execute_reply": "2022-08-12T04:30:14.486982Z"
    },
    "papermill": {
     "duration": 40.386778,
     "end_time": "2022-08-12T04:30:14.491177",
     "exception": false,
     "start_time": "2022-08-12T04:29:34.104399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install adabelief-tf --no-cache-dir \n",
    "from adabelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698bf8ad",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:14.502404Z",
     "iopub.status.busy": "2022-08-12T04:30:14.501336Z",
     "iopub.status.idle": "2022-08-12T04:30:15.783652Z",
     "shell.execute_reply": "2022-08-12T04:30:15.782662Z"
    },
    "papermill": {
     "duration": 1.290729,
     "end_time": "2022-08-12T04:30:15.786550",
     "exception": false,
     "start_time": "2022-08-12T04:30:14.495821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill as pickle   \n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import joblib\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pickle\n",
    "from pickle import load,dump\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from colorama import Fore, Back, Style\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder, PowerTransformer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import class_weight \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.layers import CuDNNLSTM\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model,model_from_json\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate, Dropout, BatchNormalization, Conv1D, Reshape, Flatten, AveragePooling1D, MaxPool1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadd9903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:15.797041Z",
     "iopub.status.busy": "2022-08-12T04:30:15.796615Z",
     "iopub.status.idle": "2022-08-12T04:30:15.813726Z",
     "shell.execute_reply": "2022-08-12T04:30:15.812779Z"
    },
    "papermill": {
     "duration": 0.025095,
     "end_time": "2022-08-12T04:30:15.816212",
     "exception": false,
     "start_time": "2022-08-12T04:30:15.791117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss = load(open('../input/standardscaler/standardscaler.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af32e096",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:15.826607Z",
     "iopub.status.busy": "2022-08-12T04:30:15.826232Z",
     "iopub.status.idle": "2022-08-12T04:30:15.844428Z",
     "shell.execute_reply": "2022-08-12T04:30:15.843479Z"
    },
    "papermill": {
     "duration": 0.025877,
     "end_time": "2022-08-12T04:30:15.846381",
     "exception": false,
     "start_time": "2022-08-12T04:30:15.820504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "    \n",
    "class MyCustomMetricCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, save_path, train=None, validation=None):\n",
    "        super(MyCustomMetricCallback, self).__init__()\n",
    "        self.train = train\n",
    "        self.validation = validation\n",
    "        self.best_score = 0\n",
    "        self.save_path =save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.train:\n",
    "            pass\n",
    "\n",
    "        if self.validation:\n",
    "            X_valid, y_valid = self.validation[0], self.validation[1]\n",
    "            y_pred = self.model.predict(X_valid).reshape( (len(X_valid), )) \n",
    "            val_score = amex_metric(y_valid, y_pred)\n",
    "            logs['my_metric_val'] = val_score\n",
    "            print(val_score)\n",
    "            if val_score>self.best_score:\n",
    "                self.y_valid = y_valid\n",
    "                self.best_pred = y_pred\n",
    "                self.best_epoch = epoch\n",
    "                self.best_score = val_score\n",
    "                self.model.save(f\"{self.save_path}.h5\")\n",
    "                print('best_val_score: ', self.best_score)\n",
    "            elif epoch-self.best_epoch > 10:\n",
    "                self.model.stop_training = True    \n",
    "            del X_valid, y_valid, y_pred, val_score\n",
    "            gc.collect()\n",
    "            \n",
    "\n",
    "ALPHA= 5\n",
    "GAMMA = 2\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3988a287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:15.857061Z",
     "iopub.status.busy": "2022-08-12T04:30:15.856197Z",
     "iopub.status.idle": "2022-08-12T04:30:19.749629Z",
     "shell.execute_reply": "2022-08-12T04:30:19.748680Z"
    },
    "papermill": {
     "duration": 3.902091,
     "end_time": "2022-08-12T04:30:19.752739",
     "exception": false,
     "start_time": "2022-08-12T04:30:15.850648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 770)]             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 770, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 770, 32)           64        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 770, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 770, 24)           792       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 770, 24)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 770, 16)           400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 770, 16)           64        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 770, 4)            68        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3080)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3080)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                49296     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 51,117\n",
      "Trainable params: 50,941\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def my_model(n_inputs):\n",
    "    \"\"\"Sequential neural network with a skip connection.\n",
    "    \n",
    "    Returns a compiled instance of tensorflow.keras.models.Model.\n",
    "    \"\"\"\n",
    "    activation = 'elu'\n",
    "    inputs = Input(shape=(n_inputs, ))\n",
    "    x = Reshape((n_inputs, 1))(inputs)\n",
    "    x = keras.layers.Conv1D(32,1,strides=1, activation=activation)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(24,1, activation=activation)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(16,1, activation=activation)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(4,1, activation=activation)(x)\n",
    "   # x = LSTM(64,return_sequences=True)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation = activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(8, activation = activation)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    gc.collect()\n",
    "    return Model(inputs, outputs)\n",
    "model = my_model(770)\n",
    "model.summary()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5769975a",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:19.763942Z",
     "iopub.status.busy": "2022-08-12T04:30:19.763497Z",
     "iopub.status.idle": "2022-08-12T04:30:19.776954Z",
     "shell.execute_reply": "2022-08-12T04:30:19.775942Z"
    },
    "papermill": {
     "duration": 0.02173,
     "end_time": "2022-08-12T04:30:19.779097",
     "exception": false,
     "start_time": "2022-08-12T04:30:19.757367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = 1\n",
    "CYCLES = 1\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 2048\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "CURRENT_FOLD = 0\n",
    "val_pred = []\n",
    "target = []\n",
    "\n",
    "def fit_model(seed, fold):\n",
    "    best_score = 0\n",
    "    train = pd.read_feather('../input/amexfeatureengineering/770_FE_train.feather')\n",
    "    FEATURES = [col for col in train.columns if col not in ['customer_ID','target']]\n",
    "    target = train.target.astype('float32')\n",
    "    train = train.loc[:,FEATURES]\n",
    "    \n",
    "    train = ss.transform(train)\n",
    "    \n",
    "    train = np.array(train)\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    idx_tr, idx_va = list(StratifiedKFold(n_splits=FOLDS, shuffle= True, random_state= seed).split(train,target))[fold]\n",
    "    X_va = train[idx_va]\n",
    "    X_tr = train[idx_tr]\n",
    "    y_tr, y_va = target[idx_tr], target[idx_va]\n",
    "    del train, target, idx_tr\n",
    "    gc.collect()\n",
    "    \n",
    "    custom = MyCustomMetricCallback(save_path = f'model_fold{fold}_seed{seed}', validation=(X_va, y_va))\n",
    "    \n",
    "    lr = ReduceLROnPlateau(monitor='my_metric_val',\n",
    "                           factor=0.2, \n",
    "                           patience=7, \n",
    "                           mode = 'max', \n",
    "                           verbose=VERBOSE)\n",
    "    callbacks = [lr, \n",
    "                 tf.keras.callbacks.TerminateOnNaN(), \n",
    "                 custom\n",
    "                ]\n",
    "    model = my_model(X_tr.shape[1])\n",
    "    model.compile(optimizer=AdaBeliefOptimizer(learning_rate=0.02,\n",
    "                                               weight_decay = 1e-5,\n",
    "                                               epsilon = 1e-7,\n",
    "        ),\n",
    "        loss=FocalLoss,\n",
    "        )\n",
    "    \n",
    "    oof = np.zeros((458913,))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    gc.collect()\n",
    "    model.fit(X_tr, y_tr, \n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=EPOCHS,\n",
    "            verbose=VERBOSE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            callbacks=callbacks)\n",
    "    \n",
    "    oof[idx_va] =  custom.best_pred\n",
    "    \n",
    "    print(f'Best Amex Score = {custom.best_score}')\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cedd0898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:19.789900Z",
     "iopub.status.busy": "2022-08-12T04:30:19.789148Z",
     "iopub.status.idle": "2022-08-12T04:30:19.794833Z",
     "shell.execute_reply": "2022-08-12T04:30:19.793864Z"
    },
    "papermill": {
     "duration": 0.013522,
     "end_time": "2022-08-12T04:30:19.797107",
     "exception": false,
     "start_time": "2022-08-12T04:30:19.783585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((458913,))\n",
    "    \n",
    "    for fold in range(FOLDS):\n",
    "        print(f'Fold {fold}_SEED {seed}')\n",
    "        oof_ = fit_model(seed, fold)\n",
    "        oof += oof_\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f421c9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T04:30:19.807816Z",
     "iopub.status.busy": "2022-08-12T04:30:19.807401Z",
     "iopub.status.idle": "2022-08-12T06:20:08.866525Z",
     "shell.execute_reply": "2022-08-12T06:20:08.865594Z"
    },
    "papermill": {
     "duration": 6592.148872,
     "end_time": "2022-08-12T06:20:11.950431",
     "exception": false,
     "start_time": "2022-08-12T04:30:19.801559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0_SEED 41\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 04:30:37.341101: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1130760400 exceeds 10% of free system memory.\n",
      "2022-08-12 04:30:38.728206: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1130760400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "180/180 [==============================] - 29s 92ms/step - loss: 0.4023 - val_loss: 0.3355\n",
      "0.7744368536692271\n",
      "best_val_score:  0.7744368536692271\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.3017 - val_loss: 0.3223\n",
      "0.7724656782483363\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2976 - val_loss: 0.3131\n",
      "0.777276986114882\n",
      "best_val_score:  0.777276986114882\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2954 - val_loss: 0.3111\n",
      "0.7865420080968879\n",
      "best_val_score:  0.7865420080968879\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2935 - val_loss: 0.2928\n",
      "0.7884580350115343\n",
      "best_val_score:  0.7884580350115343\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2931 - val_loss: 0.2937\n",
      "0.788996288983147\n",
      "best_val_score:  0.788996288983147\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2913 - val_loss: 0.2918\n",
      "0.7880915188568033\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2912 - val_loss: 0.2906\n",
      "0.7897421770785489\n",
      "best_val_score:  0.7897421770785489\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2900 - val_loss: 0.2920\n",
      "0.7886982183072613\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2894 - val_loss: 0.3088\n",
      "0.7878107265413055\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2888 - val_loss: 0.2938\n",
      "0.7911429236862546\n",
      "best_val_score:  0.7911429236862546\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2883 - val_loss: 0.2891\n",
      "0.7887657358060978\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2883 - val_loss: 0.2893\n",
      "0.7891018038731061\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2871 - val_loss: 0.2902\n",
      "0.7891390085488038\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2876 - val_loss: 0.2904\n",
      "0.7900583782159415\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2866 - val_loss: 0.2952\n",
      "0.7897934114500015\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2861 - val_loss: 0.2914\n",
      "0.7913053900650293\n",
      "best_val_score:  0.7913053900650293\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2868 - val_loss: 0.2889\n",
      "0.7904145168854402\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2858 - val_loss: 0.2919\n",
      "0.7900545502988869\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2853 - val_loss: 0.2902\n",
      "0.7910765473385897\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2856 - val_loss: 0.2924\n",
      "0.791557558816532\n",
      "best_val_score:  0.791557558816532\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2842 - val_loss: 0.2918\n",
      "0.7891662414979246\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2847 - val_loss: 0.2894\n",
      "0.789589477548187\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2844 - val_loss: 0.2943\n",
      "0.7907717312890938\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2842 - val_loss: 0.2888\n",
      "0.790405966188592\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2836 - val_loss: 0.2924\n",
      "0.7905990789091749\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2839 - val_loss: 0.2942\n",
      "0.7881339014035329\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2833 - val_loss: 0.2932\n",
      "0.7905989092261063\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2831 - val_loss: 0.2888\n",
      "0.790804714419876\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 16s 89ms/step - loss: 0.2825 - val_loss: 0.2925\n",
      "0.7880970408331581\n",
      "Best Amex Score = 0.791557558816532\n",
      "Fold 1_SEED 41\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 04:43:22.848408: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1130760400 exceeds 10% of free system memory.\n",
      "2022-08-12 04:43:24.218826: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1130760400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "180/180 [==============================] - 21s 93ms/step - loss: 0.3369 - val_loss: 0.4436\n",
      "0.7600466085195234\n",
      "best_val_score:  0.7600466085195234\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2988 - val_loss: 0.3649\n",
      "0.7769488854295965\n",
      "best_val_score:  0.7769488854295965\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2956 - val_loss: 0.3044\n",
      "0.7832747207679059\n",
      "best_val_score:  0.7832747207679059\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2935 - val_loss: 0.2980\n",
      "0.7845305524537263\n",
      "best_val_score:  0.7845305524537263\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2923 - val_loss: 0.2972\n",
      "0.7841916247776761\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2912 - val_loss: 0.3012\n",
      "0.7834328797228798\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2903 - val_loss: 0.2942\n",
      "0.7850085875030598\n",
      "best_val_score:  0.7850085875030598\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2895 - val_loss: 0.2887\n",
      "0.7874089082142142\n",
      "best_val_score:  0.7874089082142142\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2888 - val_loss: 0.2918\n",
      "0.786772146988296\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2880 - val_loss: 0.2897\n",
      "0.7857604379039758\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2875 - val_loss: 0.2917\n",
      "0.7875262995028175\n",
      "best_val_score:  0.7875262995028175\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2867 - val_loss: 0.2896\n",
      "0.7871967359107797\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2871 - val_loss: 0.2895\n",
      "0.7861645455253372\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2868 - val_loss: 0.2891\n",
      "0.7880847077381634\n",
      "best_val_score:  0.7880847077381634\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2861 - val_loss: 0.2885\n",
      "0.7879120095125446\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2862 - val_loss: 0.2894\n",
      "0.7858063444036085\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2854 - val_loss: 0.2927\n",
      "0.7867000074327415\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2852 - val_loss: 0.2886\n",
      "0.787432011062015\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2841 - val_loss: 0.2895\n",
      "0.78865268742213\n",
      "best_val_score:  0.78865268742213\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2841 - val_loss: 0.2917\n",
      "0.7871891146561121\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2846 - val_loss: 0.2976\n",
      "0.7878710990501632\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2841 - val_loss: 0.2910\n",
      "0.7871182801982946\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2834 - val_loss: 0.2884\n",
      "0.7882296499497674\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2833 - val_loss: 0.2899\n",
      "0.7861027240633617\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2832 - val_loss: 0.2903\n",
      "0.7865922607312981\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2835 - val_loss: 0.2937\n",
      "0.7854020359167939\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2827 - val_loss: 0.2908\n",
      "0.7892079829260517\n",
      "best_val_score:  0.7892079829260517\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2821 - val_loss: 0.2888\n",
      "0.7875183018600178\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2820 - val_loss: 0.2888\n",
      "0.788017358037503\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2824 - val_loss: 0.2911\n",
      "0.7863248712244675\n",
      "Best Amex Score = 0.7892079829260517\n",
      "Fold 2_SEED 41\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 04:55:00.028655: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1130760400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "180/180 [==============================] - 20s 90ms/step - loss: 0.3751 - val_loss: 0.3413\n",
      "0.7686489635882168\n",
      "best_val_score:  0.7686489635882168\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.3022 - val_loss: 0.5107\n",
      "0.7060599611636136\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2975 - val_loss: 0.3021\n",
      "0.7882512796309551\n",
      "best_val_score:  0.7882512796309551\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2953 - val_loss: 0.3157\n",
      "0.7882113457316522\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2940 - val_loss: 0.2930\n",
      "0.7920114426206408\n",
      "best_val_score:  0.7920114426206408\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2929 - val_loss: 0.2884\n",
      "0.7936190973320469\n",
      "best_val_score:  0.7936190973320469\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2918 - val_loss: 0.3188\n",
      "0.7884326963835735\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2914 - val_loss: 0.2903\n",
      "0.7941438533046503\n",
      "best_val_score:  0.7941438533046503\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2904 - val_loss: 0.3024\n",
      "0.7894536233357983\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2896 - val_loss: 0.2907\n",
      "0.7937585557645035\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2897 - val_loss: 0.2919\n",
      "0.7948276655693511\n",
      "best_val_score:  0.7948276655693511\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2893 - val_loss: 0.2882\n",
      "0.7942825658827172\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2883 - val_loss: 0.2954\n",
      "0.7922682326992642\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2883 - val_loss: 0.2918\n",
      "0.7943774656079665\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2878 - val_loss: 0.2866\n",
      "0.7948280115625459\n",
      "best_val_score:  0.7948280115625459\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2874 - val_loss: 0.2967\n",
      "0.7943909975207729\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2873 - val_loss: 0.2914\n",
      "0.7927809585286674\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2869 - val_loss: 0.2909\n",
      "0.7947226322548855\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2866 - val_loss: 0.2876\n",
      "0.793260058617516\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2856 - val_loss: 0.2861\n",
      "0.7959040902920722\n",
      "best_val_score:  0.7959040902920722\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2859 - val_loss: 0.2857\n",
      "0.7951831585678935\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2853 - val_loss: 0.2850\n",
      "0.7949281883557793\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2848 - val_loss: 0.3030\n",
      "0.7895392603886524\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2850 - val_loss: 0.2897\n",
      "0.795067473288511\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2848 - val_loss: 0.2865\n",
      "0.7937538301804097\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2843 - val_loss: 0.2960\n",
      "0.7935718827175271\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2842 - val_loss: 0.2878\n",
      "0.7938389133737512\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2835 - val_loss: 0.2886\n",
      "0.7948645703864137\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2838 - val_loss: 0.2985\n",
      "0.7949137964654742\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2834 - val_loss: 0.2879\n",
      "0.7937073809246649\n",
      "Best Amex Score = 0.7959040902920722\n",
      "Fold 3_SEED 41\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 20s 90ms/step - loss: 0.3312 - val_loss: 0.4042\n",
      "0.7787706224345505\n",
      "best_val_score:  0.7787706224345505\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.3000 - val_loss: 0.3543\n",
      "0.7668840219847588\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2965 - val_loss: 0.3402\n",
      "0.774651394226245\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2949 - val_loss: 0.3025\n",
      "0.7860344612650492\n",
      "best_val_score:  0.7860344612650492\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2928 - val_loss: 0.3456\n",
      "0.7900688883520997\n",
      "best_val_score:  0.7900688883520997\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2928 - val_loss: 0.2973\n",
      "0.7911441522448663\n",
      "best_val_score:  0.7911441522448663\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2910 - val_loss: 0.2883\n",
      "0.7942463829796496\n",
      "best_val_score:  0.7942463829796496\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2903 - val_loss: 0.2858\n",
      "0.7923617494050431\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2894 - val_loss: 0.2894\n",
      "0.7904617401099168\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2890 - val_loss: 0.2897\n",
      "0.7910093152866899\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2888 - val_loss: 0.2864\n",
      "0.7901103855835299\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2883 - val_loss: 0.2869\n",
      "0.7931972499392179\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2881 - val_loss: 0.2856\n",
      "0.7931555163835198\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2872 - val_loss: 0.2846\n",
      "0.792491537677945\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2871 - val_loss: 0.2866\n",
      "0.7922407669316476\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2869 - val_loss: 0.2863\n",
      "0.7931887205006439\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2859 - val_loss: 0.2858\n",
      "0.7925416722188492\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2863 - val_loss: 0.2858\n",
      "0.7923849756843201\n",
      "Best Amex Score = 0.7942463829796496\n",
      "Fold 4_SEED 41\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 21s 92ms/step - loss: 0.3468 - val_loss: 0.3339\n",
      "0.7819627383780523\n",
      "best_val_score:  0.7819627383780523\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2997 - val_loss: 0.3085\n",
      "0.7858071409937843\n",
      "best_val_score:  0.7858071409937843\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2966 - val_loss: 0.2913\n",
      "0.7885033992027253\n",
      "best_val_score:  0.7885033992027253\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2946 - val_loss: 0.2913\n",
      "0.7896352708636594\n",
      "best_val_score:  0.7896352708636594\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2937 - val_loss: 0.2899\n",
      "0.7896758758662985\n",
      "best_val_score:  0.7896758758662985\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2931 - val_loss: 0.2956\n",
      "0.7896468065596105\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2920 - val_loss: 0.2885\n",
      "0.7909619719485942\n",
      "best_val_score:  0.7909619719485942\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2912 - val_loss: 0.2886\n",
      "0.7908737209960166\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2908 - val_loss: 0.2876\n",
      "0.7925796522811002\n",
      "best_val_score:  0.7925796522811002\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2907 - val_loss: 0.3130\n",
      "0.7903252075713231\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2896 - val_loss: 0.2882\n",
      "0.7908603183731566\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2891 - val_loss: 0.2965\n",
      "0.7919747633265504\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2887 - val_loss: 0.2869\n",
      "0.7923861513294111\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2881 - val_loss: 0.2896\n",
      "0.7924915445423986\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2881 - val_loss: 0.2887\n",
      "0.7920345732251565\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2885 - val_loss: 0.2863\n",
      "0.7915220154234204\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2869 - val_loss: 0.2885\n",
      "0.7919440586027643\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2872 - val_loss: 0.2882\n",
      "0.7920616101816934\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2864 - val_loss: 0.2863\n",
      "0.7920629880539481\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2869 - val_loss: 0.2888\n",
      "0.7931246515924801\n",
      "best_val_score:  0.7931246515924801\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2852 - val_loss: 0.2891\n",
      "0.7926686643038543\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2852 - val_loss: 0.2866\n",
      "0.7923433857017022\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2852 - val_loss: 0.2922\n",
      "0.7909078747236441\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2853 - val_loss: 0.2879\n",
      "0.7902493125048966\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2851 - val_loss: 0.2937\n",
      "0.7913483597995534\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2848 - val_loss: 0.2881\n",
      "0.7925652171031463\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2850 - val_loss: 0.2893\n",
      "0.7923554244102935\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2849 - val_loss: 0.2884\n",
      "0.7915678253466372\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2846 - val_loss: 0.2872\n",
      "0.7923562964766064\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2841 - val_loss: 0.2864\n",
      "0.7924448760721843\n",
      "Best Amex Score = 0.7931246515924801\n",
      "Fold 0_SEED 42\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 20s 90ms/step - loss: 0.3378 - val_loss: 0.3402\n",
      "0.7794228166861877\n",
      "best_val_score:  0.7794228166861877\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2997 - val_loss: 0.3841\n",
      "0.7817809061115604\n",
      "best_val_score:  0.7817809061115604\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2953 - val_loss: 0.2962\n",
      "0.7848320854550588\n",
      "best_val_score:  0.7848320854550588\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2944 - val_loss: 0.3490\n",
      "0.783233355497045\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2933 - val_loss: 0.3108\n",
      "0.7859165300093636\n",
      "best_val_score:  0.7859165300093636\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2930 - val_loss: 0.3043\n",
      "0.7876444351806574\n",
      "best_val_score:  0.7876444351806574\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2918 - val_loss: 0.2882\n",
      "0.7887677738976905\n",
      "best_val_score:  0.7887677738976905\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2907 - val_loss: 0.2879\n",
      "0.7878374208597463\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2903 - val_loss: 0.2888\n",
      "0.7869766416185244\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2894 - val_loss: 0.3053\n",
      "0.7888194440459491\n",
      "best_val_score:  0.7888194440459491\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2894 - val_loss: 0.2910\n",
      "0.7868000869868024\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2893 - val_loss: 0.2882\n",
      "0.7897051051048374\n",
      "best_val_score:  0.7897051051048374\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2884 - val_loss: 0.2879\n",
      "0.7882880999691093\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2877 - val_loss: 0.2864\n",
      "0.7898251121445442\n",
      "best_val_score:  0.7898251121445442\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2881 - val_loss: 0.2881\n",
      "0.7908287357812297\n",
      "best_val_score:  0.7908287357812297\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2873 - val_loss: 0.2884\n",
      "0.789360052892784\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2862 - val_loss: 0.2876\n",
      "0.789333344657562\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2865 - val_loss: 0.2878\n",
      "0.7909066603456921\n",
      "best_val_score:  0.7909066603456921\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2866 - val_loss: 0.2882\n",
      "0.7903148952438548\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2861 - val_loss: 0.3049\n",
      "0.7910767021304552\n",
      "best_val_score:  0.7910767021304552\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2857 - val_loss: 0.2884\n",
      "0.7896484285105684\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2857 - val_loss: 0.2877\n",
      "0.791859014558771\n",
      "best_val_score:  0.791859014558771\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2852 - val_loss: 0.2866\n",
      "0.7906343729832268\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2850 - val_loss: 0.2900\n",
      "0.7891744854081608\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2854 - val_loss: 0.2866\n",
      "0.791004284517285\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2847 - val_loss: 0.2894\n",
      "0.788720847375739\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2847 - val_loss: 0.2932\n",
      "0.7908433449543155\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2843 - val_loss: 0.2867\n",
      "0.7898469074611352\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2846 - val_loss: 0.2871\n",
      "0.7895717075121826\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2840 - val_loss: 0.2886\n",
      "0.7891957103490077\n",
      "Best Amex Score = 0.791859014558771\n",
      "Fold 1_SEED 42\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 21s 92ms/step - loss: 0.3703 - val_loss: 0.3389\n",
      "0.7653366153035706\n",
      "best_val_score:  0.7653366153035706\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.3008 - val_loss: 0.3285\n",
      "0.7800593990225881\n",
      "best_val_score:  0.7800593990225881\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2963 - val_loss: 0.2970\n",
      "0.7860712775475902\n",
      "best_val_score:  0.7860712775475902\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2940 - val_loss: 0.2929\n",
      "0.7878215704280686\n",
      "best_val_score:  0.7878215704280686\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2927 - val_loss: 0.2896\n",
      "0.7884912242836202\n",
      "best_val_score:  0.7884912242836202\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2913 - val_loss: 0.2889\n",
      "0.7906373142607759\n",
      "best_val_score:  0.7906373142607759\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2911 - val_loss: 0.2890\n",
      "0.7901960780205008\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2896 - val_loss: 0.2896\n",
      "0.7892120111659375\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2890 - val_loss: 0.2897\n",
      "0.7915645877834709\n",
      "best_val_score:  0.7915645877834709\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2889 - val_loss: 0.2912\n",
      "0.7895460599510968\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2881 - val_loss: 0.2876\n",
      "0.7916111661524341\n",
      "best_val_score:  0.7916111661524341\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2879 - val_loss: 0.2903\n",
      "0.7925969810452617\n",
      "best_val_score:  0.7925969810452617\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2873 - val_loss: 0.2872\n",
      "0.7909035576902756\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2869 - val_loss: 0.2874\n",
      "0.791761085185763\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2863 - val_loss: 0.2880\n",
      "0.7907921677189379\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2855 - val_loss: 0.2875\n",
      "0.7920725019279651\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2856 - val_loss: 0.2893\n",
      "0.7930145713137755\n",
      "best_val_score:  0.7930145713137755\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2852 - val_loss: 0.2920\n",
      "0.7903128973982982\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2842 - val_loss: 0.2917\n",
      "0.7905629795389459\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2847 - val_loss: 0.2913\n",
      "0.7903919666128738\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2843 - val_loss: 0.2890\n",
      "0.7917948416065725\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2841 - val_loss: 0.2896\n",
      "0.7918958022415319\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2837 - val_loss: 0.2880\n",
      "0.79163096543533\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2833 - val_loss: 0.2897\n",
      "0.791448499660698\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2832 - val_loss: 0.2897\n",
      "0.7913017742627482\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2825 - val_loss: 0.2886\n",
      "0.7905565066142092\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2831 - val_loss: 0.2888\n",
      "0.7908053322083575\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2822 - val_loss: 0.2894\n",
      "0.7919769960633779\n",
      "Best Amex Score = 0.7930145713137755\n",
      "Fold 2_SEED 42\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 20s 92ms/step - loss: 0.3840 - val_loss: 0.3950\n",
      "0.7714965081638183\n",
      "best_val_score:  0.7714965081638183\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.3028 - val_loss: 0.4500\n",
      "0.7687019517132979\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2987 - val_loss: 0.3101\n",
      "0.787001127098058\n",
      "best_val_score:  0.787001127098058\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2956 - val_loss: 0.2904\n",
      "0.7889508846227626\n",
      "best_val_score:  0.7889508846227626\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2937 - val_loss: 0.2930\n",
      "0.7887542767741977\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2925 - val_loss: 0.2918\n",
      "0.7897720340338388\n",
      "best_val_score:  0.7897720340338388\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2918 - val_loss: 0.2865\n",
      "0.7907172305984186\n",
      "best_val_score:  0.7907172305984186\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2909 - val_loss: 0.2882\n",
      "0.7913601708319336\n",
      "best_val_score:  0.7913601708319336\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2898 - val_loss: 0.2871\n",
      "0.7908169008429906\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2890 - val_loss: 0.2885\n",
      "0.7904292619006861\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2884 - val_loss: 0.2912\n",
      "0.7916360875945256\n",
      "best_val_score:  0.7916360875945256\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2881 - val_loss: 0.2857\n",
      "0.7911445096217915\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2875 - val_loss: 0.2858\n",
      "0.7918135978668022\n",
      "best_val_score:  0.7918135978668022\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2870 - val_loss: 0.2889\n",
      "0.7919678242949361\n",
      "best_val_score:  0.7919678242949361\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2872 - val_loss: 0.2884\n",
      "0.7914446651225453\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2860 - val_loss: 0.2908\n",
      "0.7919296103355933\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2859 - val_loss: 0.2889\n",
      "0.7917120954590899\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2856 - val_loss: 0.2863\n",
      "0.7909896487050907\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2852 - val_loss: 0.2874\n",
      "0.7905214916360059\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2844 - val_loss: 0.2885\n",
      "0.7899145815528634\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2838 - val_loss: 0.2876\n",
      "0.7905711740496693\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2842 - val_loss: 0.2883\n",
      "0.7894512034161454\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2834 - val_loss: 0.2890\n",
      "0.791701638068442\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2830 - val_loss: 0.2916\n",
      "0.7903255568241256\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2833 - val_loss: 0.2874\n",
      "0.7918234750596025\n",
      "Best Amex Score = 0.7919678242949361\n",
      "Fold 3_SEED 42\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 21s 89ms/step - loss: 0.3524 - val_loss: 0.4586\n",
      "0.7633681752662144\n",
      "best_val_score:  0.7633681752662144\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2980 - val_loss: 0.3466\n",
      "0.7682678259653106\n",
      "best_val_score:  0.7682678259653106\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2955 - val_loss: 0.3080\n",
      "0.7780436873939589\n",
      "best_val_score:  0.7780436873939589\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2941 - val_loss: 0.2992\n",
      "0.7888809590409351\n",
      "best_val_score:  0.7888809590409351\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2925 - val_loss: 0.2915\n",
      "0.7910525149342174\n",
      "best_val_score:  0.7910525149342174\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2917 - val_loss: 0.3031\n",
      "0.7908886219759654\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2904 - val_loss: 0.3055\n",
      "0.7908800411739687\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2900 - val_loss: 0.2937\n",
      "0.7882681709107571\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2896 - val_loss: 0.2905\n",
      "0.792512600474187\n",
      "best_val_score:  0.792512600474187\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2891 - val_loss: 0.2928\n",
      "0.7921805773975183\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2888 - val_loss: 0.2887\n",
      "0.7908934647867908\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2877 - val_loss: 0.2938\n",
      "0.7914106097876095\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2879 - val_loss: 0.2913\n",
      "0.7914611572882275\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2875 - val_loss: 0.2890\n",
      "0.793733317483281\n",
      "best_val_score:  0.793733317483281\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2872 - val_loss: 0.2920\n",
      "0.7918246920314864\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2867 - val_loss: 0.2936\n",
      "0.7911298711392656\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2867 - val_loss: 0.3046\n",
      "0.789368736029194\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2858 - val_loss: 0.2895\n",
      "0.7910417984661751\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2852 - val_loss: 0.2920\n",
      "0.7920088497240774\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2852 - val_loss: 0.2890\n",
      "0.7920140369155046\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2848 - val_loss: 0.3004\n",
      "0.793155005006547\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2851 - val_loss: 0.2939\n",
      "0.7916051082287827\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2842 - val_loss: 0.2987\n",
      "0.7923302949853586\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2835 - val_loss: 0.2921\n",
      "0.7915126651136646\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2840 - val_loss: 0.2949\n",
      "0.7917973421547448\n",
      "Best Amex Score = 0.793733317483281\n",
      "Fold 4_SEED 42\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 21s 90ms/step - loss: 0.3768 - val_loss: 0.4463\n",
      "0.7038823430828419\n",
      "best_val_score:  0.7038823430828419\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2994 - val_loss: 0.3348\n",
      "0.755310181635\n",
      "best_val_score:  0.755310181635\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2963 - val_loss: 0.2980\n",
      "0.787843199970358\n",
      "best_val_score:  0.787843199970358\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2944 - val_loss: 0.3115\n",
      "0.7841637193866382\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2927 - val_loss: 0.3042\n",
      "0.7908193586231083\n",
      "best_val_score:  0.7908193586231083\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2917 - val_loss: 0.2966\n",
      "0.791395303451831\n",
      "best_val_score:  0.791395303451831\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2912 - val_loss: 0.2934\n",
      "0.7907916411902329\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2906 - val_loss: 0.2877\n",
      "0.7909381828876194\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2896 - val_loss: 0.2852\n",
      "0.7928788031304241\n",
      "best_val_score:  0.7928788031304241\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2890 - val_loss: 0.2851\n",
      "0.7923393754098251\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2883 - val_loss: 0.2863\n",
      "0.791427573757608\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2876 - val_loss: 0.2900\n",
      "0.7930590302616677\n",
      "best_val_score:  0.7930590302616677\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2876 - val_loss: 0.2892\n",
      "0.7929864064759309\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 15s 83ms/step - loss: 0.2863 - val_loss: 0.2855\n",
      "0.7936926038710832\n",
      "best_val_score:  0.7936926038710832\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2871 - val_loss: 0.2861\n",
      "0.7938999548273371\n",
      "best_val_score:  0.7938999548273371\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2859 - val_loss: 0.2932\n",
      "0.7930582125016883\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2859 - val_loss: 0.2859\n",
      "0.7923617118024765\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2853 - val_loss: 0.2872\n",
      "0.7935992140883738\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2851 - val_loss: 0.2848\n",
      "0.7940758107179098\n",
      "best_val_score:  0.7940758107179098\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2853 - val_loss: 0.2853\n",
      "0.7935842651974971\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2857 - val_loss: 0.2870\n",
      "0.7937938292143774\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2848 - val_loss: 0.2854\n",
      "0.7947672195130693\n",
      "best_val_score:  0.7947672195130693\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2842 - val_loss: 0.2884\n",
      "0.793302575521349\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 15s 86ms/step - loss: 0.2836 - val_loss: 0.2862\n",
      "0.7930461865285707\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 16s 86ms/step - loss: 0.2839 - val_loss: 0.2857\n",
      "0.794275349666228\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 15s 85ms/step - loss: 0.2834 - val_loss: 0.2863\n",
      "0.7941687961834369\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2835 - val_loss: 0.2851\n",
      "0.7940926860180304\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 15s 84ms/step - loss: 0.2837 - val_loss: 0.2855\n",
      "0.7950687568272597\n",
      "best_val_score:  0.7950687568272597\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 16s 88ms/step - loss: 0.2830 - val_loss: 0.2873\n",
      "0.7937733500332009\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 16s 87ms/step - loss: 0.2828 - val_loss: 0.2859\n",
      "0.7940472985034586\n",
      "Best Amex Score = 0.7950687568272597\n"
     ]
    }
   ],
   "source": [
    "SEED = [41,42]  #<-- Update\n",
    "oof = np.zeros((458913,))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_ = run_k_fold(FOLDS, seed)\n",
    "    oof += oof_ / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d17238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:17.735036Z",
     "iopub.status.busy": "2022-08-12T06:20:17.734396Z",
     "iopub.status.idle": "2022-08-12T06:20:19.709834Z",
     "shell.execute_reply": "2022-08-12T06:20:19.708959Z"
    },
    "papermill": {
     "duration": 4.785341,
     "end_time": "2022-08-12T06:20:19.712217",
     "exception": false,
     "start_time": "2022-08-12T06:20:14.926876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_feather('../input/amexfeatureengineering/770_FE_train.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4dc9bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:25.737859Z",
     "iopub.status.busy": "2022-08-12T06:20:25.736737Z",
     "iopub.status.idle": "2022-08-12T06:20:25.742181Z",
     "shell.execute_reply": "2022-08-12T06:20:25.741345Z"
    },
    "papermill": {
     "duration": 2.902696,
     "end_time": "2022-08-12T06:20:25.744132",
     "exception": false,
     "start_time": "2022-08-12T06:20:22.841436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9182996b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:31.408596Z",
     "iopub.status.busy": "2022-08-12T06:20:31.408202Z",
     "iopub.status.idle": "2022-08-12T06:20:31.682561Z",
     "shell.execute_reply": "2022-08-12T06:20:31.681644Z"
    },
    "papermill": {
     "duration": 3.145434,
     "end_time": "2022-08-12T06:20:31.685363",
     "exception": false,
     "start_time": "2022-08-12T06:20:28.539929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_Amex = amex_metric(target,oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9cb111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:37.955830Z",
     "iopub.status.busy": "2022-08-12T06:20:37.955457Z",
     "iopub.status.idle": "2022-08-12T06:20:37.962735Z",
     "shell.execute_reply": "2022-08-12T06:20:37.962000Z"
    },
    "papermill": {
     "duration": 2.774393,
     "end_time": "2022-08-12T06:20:37.964986",
     "exception": false,
     "start_time": "2022-08-12T06:20:35.190593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7938481749446653"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_Amex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4e12ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:44.035724Z",
     "iopub.status.busy": "2022-08-12T06:20:44.035027Z",
     "iopub.status.idle": "2022-08-12T06:20:44.042705Z",
     "shell.execute_reply": "2022-08-12T06:20:44.041671Z"
    },
    "papermill": {
     "duration": 3.231744,
     "end_time": "2022-08-12T06:20:44.045968",
     "exception": false,
     "start_time": "2022-08-12T06:20:40.814224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61192226, 0.04923945, 0.06925567, ..., 0.68758154, 0.16656805,\n",
       "       0.66501907])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c16b7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T06:20:49.802792Z",
     "iopub.status.busy": "2022-08-12T06:20:49.802426Z",
     "iopub.status.idle": "2022-08-12T06:20:50.986692Z",
     "shell.execute_reply": "2022-08-12T06:20:50.985700Z"
    },
    "papermill": {
     "duration": 4.087138,
     "end_time": "2022-08-12T06:20:50.989577",
     "exception": false,
     "start_time": "2022-08-12T06:20:46.902439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = pd.DataFrame({'customer_ID':train.customer_ID,'target':train.target,'oof_pred':oof})\n",
    "oof.to_csv('oof_CNN1D.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e6f8d",
   "metadata": {
    "papermill": {
     "duration": 2.946467,
     "end_time": "2022-08-12T06:20:57.090314",
     "exception": false,
     "start_time": "2022-08-12T06:20:54.143847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6698.053766,
   "end_time": "2022-08-12T06:21:03.183879",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-12T04:29:25.130113",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
